{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of supported layers\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Activation,\n",
    "    AveragePooling2D,\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    GlobalAveragePooling2D,\n",
    "    GlobalMaxPooling2D,\n",
    "    Lambda, # only for polynomial activation in the form of `Lambda(lambda x: x**2+x)`\n",
    "    MaxPooling2D,\n",
    "    ReLU,\n",
    "    Softmax,\n",
    "    )\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y_train and y_test to one-hot encoding\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X_train and X_test to 4D tensor\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(28,28,1))\n",
    "out = Conv2D(4, 3, use_bias=False)(inputs)\n",
    "out = BatchNormalization()(out)\n",
    "out = Lambda(lambda x: x**2+x)(out) # best practice: use polynomial activation instead of ReLU\n",
    "out = AveragePooling2D()(out) # best practice: use AveragePooling2D instead of MaxPooling2D\n",
    "out = Conv2D(16, 3, use_bias=False)(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Lambda(lambda x: x**2+x)(out)\n",
    "out = AveragePooling2D()(out)\n",
    "out = GlobalAveragePooling2D()(out) # best practice: use GlobalAveragePooling2D instead of Flatten\n",
    "out = Dense(10, activation=None)(out)\n",
    "out = Softmax()(out)\n",
    "model = Model(inputs, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 26, 26, 4)         36        \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 26, 26, 4)        16        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 26, 26, 4)         0         \n",
      "                                                                 \n",
      " average_pooling2d (AverageP  (None, 13, 13, 4)        0         \n",
      " ooling2D)                                                       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 16)        576       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 11, 11, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lambda_1 (Lambda)           (None, 11, 11, 16)        0         \n",
      "                                                                 \n",
      " average_pooling2d_1 (Averag  (None, 5, 5, 16)         0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 16)               0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                170       \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 862\n",
      "Trainable params: 822\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['acc']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 21:31:00.844832: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 12s 23ms/step - loss: 1.6842 - acc: 0.5150 - val_loss: 1.0291 - val_acc: 0.7401\n",
      "Epoch 2/100\n",
      "469/469 [==============================] - 13s 28ms/step - loss: 0.7740 - acc: 0.8326 - val_loss: 0.5402 - val_acc: 0.8735\n",
      "Epoch 3/100\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 0.4480 - acc: 0.9004 - val_loss: 0.3657 - val_acc: 0.9106\n",
      "Epoch 4/100\n",
      "469/469 [==============================] - 14s 30ms/step - loss: 0.3179 - acc: 0.9244 - val_loss: 0.2855 - val_acc: 0.9289\n",
      "Epoch 5/100\n",
      "469/469 [==============================] - 14s 29ms/step - loss: 0.2529 - acc: 0.9365 - val_loss: 0.2198 - val_acc: 0.9443\n",
      "Epoch 6/100\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.2135 - acc: 0.9436 - val_loss: 0.1831 - val_acc: 0.9532\n",
      "Epoch 7/100\n",
      "469/469 [==============================] - 11s 24ms/step - loss: 0.1879 - acc: 0.9488 - val_loss: 0.1799 - val_acc: 0.9508\n",
      "Epoch 8/100\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.1734 - acc: 0.9516 - val_loss: 0.1559 - val_acc: 0.9562\n",
      "Epoch 9/100\n",
      "469/469 [==============================] - 9s 20ms/step - loss: 0.1591 - acc: 0.9546 - val_loss: 0.1517 - val_acc: 0.9560\n",
      "Epoch 10/100\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.1504 - acc: 0.9564 - val_loss: 0.1469 - val_acc: 0.9569\n",
      "Epoch 11/100\n",
      "469/469 [==============================] - 10s 21ms/step - loss: 0.1429 - acc: 0.9581 - val_loss: 0.1556 - val_acc: 0.9520\n",
      "Epoch 12/100\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.1367 - acc: 0.9598 - val_loss: 0.1309 - val_acc: 0.9606\n",
      "Epoch 13/100\n",
      "469/469 [==============================] - 14s 30ms/step - loss: 0.1330 - acc: 0.9610 - val_loss: 0.1294 - val_acc: 0.9617\n",
      "Epoch 14/100\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.1278 - acc: 0.9619 - val_loss: 0.1364 - val_acc: 0.9620\n",
      "Epoch 15/100\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.1257 - acc: 0.9628 - val_loss: 0.1275 - val_acc: 0.9619\n",
      "Epoch 16/100\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1216 - acc: 0.9636 - val_loss: 0.1294 - val_acc: 0.9628\n",
      "Epoch 17/100\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.1186 - acc: 0.9644 - val_loss: 0.1165 - val_acc: 0.9665\n",
      "Epoch 18/100\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.1177 - acc: 0.9644 - val_loss: 0.1401 - val_acc: 0.9579\n",
      "Epoch 19/100\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.1150 - acc: 0.9653 - val_loss: 0.1102 - val_acc: 0.9658\n",
      "Epoch 20/100\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.1127 - acc: 0.9661 - val_loss: 0.1191 - val_acc: 0.9643\n",
      "Epoch 21/100\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.1116 - acc: 0.9657 - val_loss: 0.2109 - val_acc: 0.9346\n",
      "Epoch 22/100\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.1107 - acc: 0.9658 - val_loss: 0.1225 - val_acc: 0.9615\n",
      "Epoch 23/100\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.1088 - acc: 0.9676 - val_loss: 0.1174 - val_acc: 0.9639\n",
      "Epoch 24/100\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.1079 - acc: 0.9670 - val_loss: 0.1051 - val_acc: 0.9685\n",
      "Epoch 25/100\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.1064 - acc: 0.9681 - val_loss: 0.1005 - val_acc: 0.9692\n",
      "Epoch 26/100\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.1055 - acc: 0.9676 - val_loss: 0.1030 - val_acc: 0.9680\n",
      "Epoch 27/100\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.1045 - acc: 0.9681 - val_loss: 0.1050 - val_acc: 0.9681\n",
      "Epoch 28/100\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.1039 - acc: 0.9688 - val_loss: 0.1073 - val_acc: 0.9666\n",
      "Epoch 29/100\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.1020 - acc: 0.9689 - val_loss: 0.1198 - val_acc: 0.9650\n",
      "Epoch 30/100\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.1016 - acc: 0.9689 - val_loss: 0.1138 - val_acc: 0.9638\n",
      "Epoch 31/100\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.1011 - acc: 0.9688 - val_loss: 0.1070 - val_acc: 0.9668\n",
      "Epoch 32/100\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0995 - acc: 0.9701 - val_loss: 0.0953 - val_acc: 0.9706\n",
      "Epoch 33/100\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0996 - acc: 0.9693 - val_loss: 0.0954 - val_acc: 0.9716\n",
      "Epoch 34/100\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0982 - acc: 0.9704 - val_loss: 0.0977 - val_acc: 0.9702\n",
      "Epoch 35/100\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0983 - acc: 0.9699 - val_loss: 0.1100 - val_acc: 0.9679\n",
      "Epoch 36/100\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0972 - acc: 0.9704 - val_loss: 0.1159 - val_acc: 0.9652\n",
      "Epoch 37/100\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0962 - acc: 0.9705 - val_loss: 0.0952 - val_acc: 0.9709\n",
      "Epoch 38/100\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0953 - acc: 0.9709 - val_loss: 0.0895 - val_acc: 0.9728\n",
      "Epoch 39/100\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0957 - acc: 0.9705 - val_loss: 0.1069 - val_acc: 0.9668\n",
      "Epoch 40/100\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0947 - acc: 0.9705 - val_loss: 0.0912 - val_acc: 0.9719\n",
      "Epoch 41/100\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.0931 - acc: 0.9718 - val_loss: 0.0952 - val_acc: 0.9723\n",
      "Epoch 42/100\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.0943 - acc: 0.9712 - val_loss: 0.1078 - val_acc: 0.9678\n",
      "Epoch 43/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0938 - acc: 0.9708 - val_loss: 0.0958 - val_acc: 0.9702\n",
      "Epoch 44/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0929 - acc: 0.9707 - val_loss: 0.1011 - val_acc: 0.9675\n",
      "Epoch 45/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0920 - acc: 0.9713 - val_loss: 0.1134 - val_acc: 0.9640\n",
      "Epoch 46/100\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.0927 - acc: 0.9716 - val_loss: 0.1082 - val_acc: 0.9655\n",
      "Epoch 47/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0919 - acc: 0.9721 - val_loss: 0.0923 - val_acc: 0.9721\n",
      "Epoch 48/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0915 - acc: 0.9715 - val_loss: 0.1072 - val_acc: 0.9666\n",
      "Epoch 49/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0910 - acc: 0.9719 - val_loss: 0.0911 - val_acc: 0.9709\n",
      "Epoch 50/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0921 - acc: 0.9717 - val_loss: 0.0891 - val_acc: 0.9722\n",
      "Epoch 51/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0904 - acc: 0.9719 - val_loss: 0.1168 - val_acc: 0.9668\n",
      "Epoch 52/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0886 - acc: 0.9731 - val_loss: 0.0947 - val_acc: 0.9693\n",
      "Epoch 53/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0902 - acc: 0.9723 - val_loss: 0.0882 - val_acc: 0.9728\n",
      "Epoch 54/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0882 - acc: 0.9724 - val_loss: 0.0898 - val_acc: 0.9733\n",
      "Epoch 55/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0877 - acc: 0.9729 - val_loss: 0.0946 - val_acc: 0.9708\n",
      "Epoch 56/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0886 - acc: 0.9725 - val_loss: 0.0981 - val_acc: 0.9695\n",
      "Epoch 57/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0883 - acc: 0.9726 - val_loss: 0.0845 - val_acc: 0.9739\n",
      "Epoch 58/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0880 - acc: 0.9727 - val_loss: 0.1169 - val_acc: 0.9635\n",
      "Epoch 59/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0888 - acc: 0.9724 - val_loss: 0.0858 - val_acc: 0.9738\n",
      "Epoch 60/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0873 - acc: 0.9728 - val_loss: 0.0905 - val_acc: 0.9724\n",
      "Epoch 61/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0878 - acc: 0.9729 - val_loss: 0.0816 - val_acc: 0.9749\n",
      "Epoch 62/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0867 - acc: 0.9734 - val_loss: 0.0844 - val_acc: 0.9733\n",
      "Epoch 63/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0878 - acc: 0.9728 - val_loss: 0.0931 - val_acc: 0.9717\n",
      "Epoch 64/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0870 - acc: 0.9725 - val_loss: 0.0920 - val_acc: 0.9723\n",
      "Epoch 65/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0861 - acc: 0.9731 - val_loss: 0.0908 - val_acc: 0.9719\n",
      "Epoch 66/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0865 - acc: 0.9734 - val_loss: 0.0903 - val_acc: 0.9727\n",
      "Epoch 67/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0854 - acc: 0.9738 - val_loss: 0.0829 - val_acc: 0.9750\n",
      "Epoch 68/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0842 - acc: 0.9740 - val_loss: 0.1045 - val_acc: 0.9684\n",
      "Epoch 69/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0857 - acc: 0.9733 - val_loss: 0.0873 - val_acc: 0.9743\n",
      "Epoch 70/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0845 - acc: 0.9736 - val_loss: 0.0886 - val_acc: 0.9719\n",
      "Epoch 71/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0845 - acc: 0.9733 - val_loss: 0.1150 - val_acc: 0.9642\n",
      "Epoch 72/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0847 - acc: 0.9738 - val_loss: 0.0915 - val_acc: 0.9718\n",
      "Epoch 73/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0848 - acc: 0.9733 - val_loss: 0.0952 - val_acc: 0.9695\n",
      "Epoch 74/100\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0849 - acc: 0.9735 - val_loss: 0.1147 - val_acc: 0.9656\n",
      "Epoch 75/100\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0843 - acc: 0.9733 - val_loss: 0.1071 - val_acc: 0.9682\n",
      "Epoch 76/100\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0841 - acc: 0.9734 - val_loss: 0.0947 - val_acc: 0.9701\n",
      "Epoch 77/100\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0842 - acc: 0.9745 - val_loss: 0.0953 - val_acc: 0.9690\n",
      "Epoch 78/100\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0829 - acc: 0.9738 - val_loss: 0.0943 - val_acc: 0.9717\n",
      "Epoch 79/100\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0817 - acc: 0.9743 - val_loss: 0.0930 - val_acc: 0.9713\n",
      "Epoch 80/100\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0832 - acc: 0.9739 - val_loss: 0.0915 - val_acc: 0.9718\n",
      "Epoch 81/100\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0829 - acc: 0.9741 - val_loss: 0.0891 - val_acc: 0.9711\n",
      "Epoch 82/100\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0822 - acc: 0.9739 - val_loss: 0.0932 - val_acc: 0.9717\n",
      "Epoch 83/100\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 0.0827 - acc: 0.9735 - val_loss: 0.0854 - val_acc: 0.9725\n",
      "Epoch 84/100\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.0835 - acc: 0.9743 - val_loss: 0.1205 - val_acc: 0.9626\n",
      "Epoch 85/100\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0829 - acc: 0.9740 - val_loss: 0.1037 - val_acc: 0.9673\n",
      "Epoch 86/100\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 0.0822 - acc: 0.9745 - val_loss: 0.0859 - val_acc: 0.9731\n",
      "Epoch 87/100\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 0.0817 - acc: 0.9747 - val_loss: 0.1015 - val_acc: 0.9699\n",
      "Epoch 88/100\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0814 - acc: 0.9745 - val_loss: 0.0895 - val_acc: 0.9708\n",
      "Epoch 89/100\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0814 - acc: 0.9746 - val_loss: 0.0817 - val_acc: 0.9727\n",
      "Epoch 90/100\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0826 - acc: 0.9742 - val_loss: 0.0881 - val_acc: 0.9725\n",
      "Epoch 91/100\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0813 - acc: 0.9745 - val_loss: 0.0883 - val_acc: 0.9719\n",
      "Epoch 92/100\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0818 - acc: 0.9746 - val_loss: 0.1081 - val_acc: 0.9661\n",
      "Epoch 93/100\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0798 - acc: 0.9750 - val_loss: 0.0848 - val_acc: 0.9733\n",
      "Epoch 94/100\n",
      "469/469 [==============================] - 9s 20ms/step - loss: 0.0813 - acc: 0.9744 - val_loss: 0.0878 - val_acc: 0.9725\n",
      "Epoch 95/100\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.0810 - acc: 0.9750 - val_loss: 0.1085 - val_acc: 0.9677\n",
      "Epoch 96/100\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.0801 - acc: 0.9749 - val_loss: 0.0831 - val_acc: 0.9738\n",
      "Epoch 97/100\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.0804 - acc: 0.9748 - val_loss: 0.1115 - val_acc: 0.9650\n",
      "Epoch 98/100\n",
      "469/469 [==============================] - 11s 22ms/step - loss: 0.0806 - acc: 0.9748 - val_loss: 0.0880 - val_acc: 0.9722\n",
      "Epoch 99/100\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.0800 - acc: 0.9748 - val_loss: 0.0807 - val_acc: 0.9741\n",
      "Epoch 100/100\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 0.0799 - acc: 0.9747 - val_loss: 0.0907 - val_acc: 0.9728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17f702c10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('best_practice.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x17f5d1670>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Model(model.input, model.layers[-2].output)\n",
    "model2.layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 155ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -6.9723616,  -7.2022758,  -1.5148271,  -5.828606 ,  -8.659952 ,\n",
       "         -6.311898 , -20.928083 ,  11.584262 , -13.573946 ,  -0.0604342]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X_test[[0]]\n",
    "y = model2.predict(X)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputLayer {'batch_input_shape': (None, 28, 28, 1), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'input_1'}\n",
      "(None, 28, 28, 1) (None, 28, 28, 1)\n",
      "Conv2D {'name': 'conv2d', 'trainable': True, 'dtype': 'float32', 'filters': 4, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': False, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "(None, 28, 28, 1) (None, 26, 26, 4)\n",
      "(3, 3, 1, 4)\n",
      "BatchNormalization {'name': 'batch_normalization', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}\n",
      "(None, 26, 26, 4) (None, 26, 26, 4)\n",
      "(4,)\n",
      "(4,)\n",
      "Lambda {'name': 'lambda', 'trainable': True, 'dtype': 'float32', 'function': ('4wEAAAAAAAAAAAAAAAEAAAACAAAAQwAAAHMMAAAAfABkARMAfAAXAFMAKQJO6QIAAACpACkB2gF4\\ncgIAAAByAgAAAPpOL3Zhci9mb2xkZXJzL2d0L3NnM3Y4cmQxM2w1Mmp4OTFtZmJnemJmYzAwMDBn\\nbi9UL2lweWtlcm5lbF84ODczNi8yMTc2NzAzOTE5LnB52gg8bGFtYmRhPgQAAADzAAAAAA==\\n', None, None), 'function_type': 'lambda', 'module': '__main__', 'output_shape': None, 'output_shape_type': 'raw', 'output_shape_module': None, 'arguments': {}}\n",
      "('4wEAAAAAAAAAAAAAAAEAAAACAAAAQwAAAHMMAAAAfABkARMAfAAXAFMAKQJO6QIAAACpACkB2gF4\\ncgIAAAByAgAAAPpOL3Zhci9mb2xkZXJzL2d0L3NnM3Y4cmQxM2w1Mmp4OTFtZmJnemJmYzAwMDBn\\nbi9UL2lweWtlcm5lbF84ODczNi8yMTc2NzAzOTE5LnB52gg8bGFtYmRhPgQAAADzAAAAAA==\\n', None, None)\n",
      "(None, 26, 26, 4) (None, 26, 26, 4)\n",
      "AveragePooling2D {'name': 'average_pooling2d', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}\n",
      "(None, 26, 26, 4) (None, 13, 13, 4)\n",
      "Conv2D {'name': 'conv2d_1', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': False, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "(None, 13, 13, 4) (None, 11, 11, 16)\n",
      "(3, 3, 4, 16)\n",
      "BatchNormalization {'name': 'batch_normalization_1', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}\n",
      "(None, 11, 11, 16) (None, 11, 11, 16)\n",
      "(16,)\n",
      "(16,)\n",
      "Lambda {'name': 'lambda_1', 'trainable': True, 'dtype': 'float32', 'function': ('4wEAAAAAAAAAAAAAAAEAAAACAAAAQwAAAHMMAAAAfABkARMAfAAXAFMAKQJO6QIAAACpACkB2gF4\\ncgIAAAByAgAAAPpOL3Zhci9mb2xkZXJzL2d0L3NnM3Y4cmQxM2w1Mmp4OTFtZmJnemJmYzAwMDBn\\nbi9UL2lweWtlcm5lbF84ODczNi8yMTc2NzAzOTE5LnB52gg8bGFtYmRhPggAAADzAAAAAA==\\n', None, None), 'function_type': 'lambda', 'module': '__main__', 'output_shape': None, 'output_shape_type': 'raw', 'output_shape_module': None, 'arguments': {}}\n",
      "('4wEAAAAAAAAAAAAAAAEAAAACAAAAQwAAAHMMAAAAfABkARMAfAAXAFMAKQJO6QIAAACpACkB2gF4\\ncgIAAAByAgAAAPpOL3Zhci9mb2xkZXJzL2d0L3NnM3Y4cmQxM2w1Mmp4OTFtZmJnemJmYzAwMDBn\\nbi9UL2lweWtlcm5lbF84ODczNi8yMTc2NzAzOTE5LnB52gg8bGFtYmRhPggAAADzAAAAAA==\\n', None, None)\n",
      "(None, 11, 11, 16) (None, 11, 11, 16)\n",
      "AveragePooling2D {'name': 'average_pooling2d_1', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}\n",
      "(None, 11, 11, 16) (None, 5, 5, 16)\n",
      "GlobalAveragePooling2D {'name': 'global_average_pooling2d', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last', 'keepdims': False}\n",
      "(None, 5, 5, 16) (None, 16)\n",
      "Dense {'name': 'dense', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "(None, 16) (None, 10)\n",
      "(16, 10)\n",
      "(10,)\n",
      "Softmax {'name': 'softmax', 'trainable': True, 'dtype': 'float32', 'axis': -1}\n",
      "(None, 10) (None, 10)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.__class__.__name__, layer.get_config())\n",
    "    try:\n",
    "        print(layer.get_config()['function'])\n",
    "    except:\n",
    "        pass\n",
    "    print(layer.get_input_shape_at(0),layer.get_output_shape_at(0))\n",
    "    try:\n",
    "        print(layer.get_weights()[0].shape)\n",
    "        print(layer.get_weights()[1].shape)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"best_practice.json\", \"w\") as f:\n",
    "    json.dump({'X': X.flatten().tolist(), 'y': y.flatten().tolist()}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras2circom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71414dc221f26c27f268040756e42b4f7499507456a67f7434828e3314a20678"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
